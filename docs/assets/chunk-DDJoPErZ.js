const e='---\r\nid: 98\r\nSection: Case Studies\r\nslug: safely-launching-new-markhazleton-com\r\nname: Safely Launching a New MarkHazleton.com\r\ndescription: A technical case study on launching a modern React-based static site while ensuring SEO crawlability, implementing deployment validation, and safely migrating a production domain.\r\nkeywords: Mark Hazleton, static site deployment, SEO crawlability, Azure Static Web Apps, React SSR, Screaming Frog, technical migration, build versioning\r\nimg_src: /img/ArgostoliGreeceBeach.jpg\r\nlastmod: 2026-01-20\r\npublishedDate: 2026-01-20\r\nestimatedReadTime: 12\r\nchangefreq: monthly\r\nsubtitle: Technical Challenges and Solutions in Modern Static Site Deployment\r\nauthor: Mark Hazleton\r\nsummary: A detailed account of migrating MarkHazleton.com to a modern React-based static site, solving critical SEO crawlability issues, implementing build tracking, and safely switching production domains between Azure Static Web Apps.\r\nconclusionTitle: Lessons Learned\r\nconclusionSummary: Modern static sites require careful attention to pre-rendering, crawler compatibility, and deployment validation. Simple solutions like hidden navigation fallbacks and build versioning can prevent production issues.\r\nconclusionKeyHeading: Bottom Line\r\nconclusionKeyText: Pre-rendered content means nothing if crawlers can\'t parse it. Always validate with actual SEO tools before going live.\r\nconclusionText: Building a beautiful, fast site is only half the battle. Ensuring search engines can crawl it, tracking deployments accurately, and migrating safely are equally critical. Test with real tools, implement fallbacks, and always have a way to verify what\'s in production.\r\nseo:\r\n  title: Safely Launching a New MarkHazleton.com - Technical Case Study\r\n  titleSuffix: \r\n  description: Technical case study on launching a React-based static site with solutions for SEO crawlability, build tracking, and Azure Static Web Apps domain migration.\r\n  keywords: static site deployment, SEO crawlability, Azure Static Web Apps, React pre-rendering, Screaming Frog, build versioning, domain migration\r\n  canonical: https://markhazleton.com/blog/safely-launching-new-markhazleton-com\r\n  robots: index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1\r\nog:\r\n  title: Safely Launching a New MarkHazleton.com - Technical Case Study\r\n  description: How I rebuilt my site with React, fixed critical SEO crawlability issues, and safely migrated production domains on Azure Static Web Apps.\r\n  type: article\r\n  image: /img/ArgostoliGreeceBeach.jpg\r\n  imageAlt: Safely Launching a New MarkHazleton.com - Mark Hazleton\r\ntwitter:\r\n  title: Technical Case Study - Launching a New Static Site\r\n  description: Solving SEO crawlability, implementing build tracking, and migrating production domains safely on Azure Static Web Apps.\r\n  image: /img/ArgostoliGreeceBeach.jpg\r\n  imageAlt: Safely Launching a New MarkHazleton.com - Mark Hazleton\r\nyoutubeUrl: null\r\nyoutubeTitle: null\r\n---\r\n\r\n# Safely Launching a New MarkHazleton.com\r\n\r\n## The Challenge: When Your Beautiful Site Is Invisible to Crawlers\r\n\r\nThe new site looked perfect. React + Vite for blazing-fast performance. Pre-rendered static HTML for every page. Clean URLs. A proper sitemap. Everything Google\'s documentation said to do.\r\n\r\nThen I ran Screaming Frog SEO Spider against it. The homepage showed **0 outlinks**. Zero. As if the navigation didn\'t exist.\r\n\r\nThis is the story of rebuilding MarkHazleton.com with modern tools, discovering a critical SEO issue hours before launch, and implementing solutions that work for both human visitors and non-JavaScript crawlers.\r\n\r\n## Why Rework the Site?\r\n\r\nThe old site worked. It had content. It ranked well. But every time I looked at it, something felt off.\r\n\r\n**It felt like a marketing sales pitch.** Every paragraph seemed designed to sell something rather than share something. I didn\'t want to convince anyone of anything—I just wanted to tell my developer story in a clean, easy-to-read way.\r\n\r\n### Saying Goodbye to Pug\r\n\r\nThe old site was built with Pug (formerly Jade) templates. For those unfamiliar, Pug uses indentation-based syntax that compiles to HTML:\r\n\r\n```pug\r\n.article\r\n  h1.title= article.title\r\n  .content\r\n    p= article.description\r\n```\r\n\r\nPug was fine when I set it up years ago. But editing content meant navigating template syntax, understanding indentation rules, and keeping data separate from presentation. Want to update an article? Edit a JSON file, maybe a Pug template, remember the build process, hope nothing broke.\r\n\r\n### The Markdown Revelation\r\n\r\nI wanted to write like a developer, not maintain a template system. Markdown with frontmatter metadata solved this perfectly:\r\n\r\n```markdown\r\n---\r\ntitle: My Article Title\r\npublishedDate: 2026-01-20\r\ndescription: This is what the article is about\r\n---\r\n\r\n# My Article Title\r\n\r\nActual content goes here. No template syntax. No build system quirks.\r\nJust write.\r\n```\r\n\r\nAll the metadata at the top of the document. The content below in clean, readable Markdown. Edit in any text editor. Preview anywhere. Git diffs that make sense.\r\n\r\nThis isn\'t revolutionary—it\'s what most modern static site generators use. But for someone who\'d been maintaining a custom Pug-based system for years, it felt like coming home.\r\n\r\n### The Real Goal\r\n\r\nI wanted a site that felt like me:\r\n- **Direct** - Say what I mean without marketing speak\r\n- **Technical** - Show real code, real problems, real solutions\r\n- **Honest** - Document failures alongside successes\r\n- **Maintainable** - Easy to update without remembering custom build quirks\r\n\r\nThe React + Vite + Markdown stack delivered all of this. The site looks cleaner, reads better, and—most importantly—I actually want to write for it now.\r\n\r\n## The Architecture: Modern Static Site with React\r\n\r\n### The Build Process\r\n\r\nThe site uses a sophisticated pre-rendering approach:\r\n\r\n1. **Vite** builds the React application for optimal performance\r\n2. **React Router** handles client-side routing\r\n3. **Custom pre-render script** generates static HTML for all routes\r\n4. **Azure Static Web Apps** hosts everything with automatic deployments\r\n\r\nThe pre-render script crawls every route and saves fully-rendered HTML:\r\n\r\n```javascript\r\n// Simplified version of the prerender process\r\nconst routes = [\'/\', \'/blog\', \'/projects\', \'/github\', \'/videos\', \'/contact\'];\r\n\r\nfor (const route of routes) {\r\n  const html = await renderToString(<App location={route} />);\r\n  await fs.writeFile(`docs${route}/index.html`, html);\r\n}\r\n```\r\n\r\nEvery page exists as a complete, pre-rendered HTML file. Perfect for crawlers, right?\r\n\r\n## The Problem: Crawlers Couldn\'t See the Links\r\n\r\n### Discovery with Screaming Frog\r\n\r\nWhen I tested the staging site with Screaming Frog SEO Spider (a tool that crawls websites the way Google does), it only discovered the homepage. None of the navigation links. None of the blog posts. Just one page.\r\n\r\nLooking at the HTML source, all the links were there:\r\n\r\n```html\r\n<a href="/blog" data-discover="true">Blog</a>\r\n<a href="/projects" data-discover="true">Projects</a>\r\n<a href="/github" data-discover="true">GitHub</a>\r\n```\r\n\r\nBut Screaming Frog showed 0 outlinks from the homepage.\r\n\r\n### Root Cause Analysis\r\n\r\nThe issue wasn\'t the links themselves—it was how the HTML was structured. React\'s `renderToString` outputs everything on a single, extremely long line. The entire `<div id="root">` content with all navigation, all links, all content—one massive line of minified HTML.\r\n\r\nWhile browsers handle this fine, some crawlers struggle to parse HTML that isn\'t properly formatted. Screaming Frog was failing to extract links from this compressed structure, especially from the homepage where the initial React shell lived.\r\n\r\n**The critical insight:** Pre-rendered HTML means nothing if crawlers can\'t parse it.\r\n\r\n## The Solutions: Making Crawlers Happy\r\n\r\n### Solution 1: Crawler-Friendly Fallback Navigation\r\n\r\nI added a hidden navigation block at the end of the HTML template with plain, simple links:\r\n\r\n```html\r\n\x3c!-- Crawler-friendly navigation links (hidden from visual browsers) --\x3e\r\n<nav aria-hidden="true" style="position:absolute;left:-9999px;top:-9999px;">\r\n  <a href="/">Home</a>\r\n  <a href="/blog">Blog</a>\r\n  <a href="/projects">Projects</a>\r\n  <a href="/github">GitHub</a>\r\n  <a href="/videos">Videos</a>\r\n  <a href="/contact">Contact</a>\r\n  <a href="/sitemap">Site Map</a>\r\n</nav>\r\n```\r\n\r\nThis navigation:\r\n- Is positioned off-screen (invisible to sighted users)\r\n- Uses `aria-hidden="true"` (invisible to screen readers)\r\n- Contains plain HTML links with no React attributes\r\n- Lives outside the React root element\r\n- Provides a fallback for crawlers that struggle with minified HTML\r\n\r\n**Result:** Screaming Frog immediately found all links and started crawling the entire site.\r\n\r\n### Solution 2: HTML Sitemap Page\r\n\r\nI created a user-friendly HTML sitemap at `/sitemap` that lists every page:\r\n\r\n```typescript\r\n// Sitemap page component structure\r\n<Layout>\r\n  <section>\r\n    <h2>Main Pages</h2>\r\n    <nav>\r\n      {mainPages.map(page => <Link to={page.path}>{page.label}</Link>)}\r\n    </nav>\r\n\r\n    <h2>Projects ({projects.length})</h2>\r\n    {projects.map(project => <Link to={`/projects/${project.slug}`}>...)}\r\n\r\n    <h2>Blog Posts by Section ({posts.length})</h2>\r\n    {Object.entries(postsBySection).map(([section, posts]) => (\r\n      <div>\r\n        <h3>{section} ({posts.length})</h3>\r\n        {posts.map(post => <Link to={`/blog/${post.slug}`}>...)}\r\n      </div>\r\n    ))}\r\n  </section>\r\n</Layout>\r\n```\r\n\r\nThis provides:\r\n- A single page with links to all content\r\n- Human-readable structure grouped by category\r\n- Fallback for crawlers that need explicit discovery\r\n- User-friendly navigation aid\r\n\r\n### Solution 3: Fixed Azure Static Web Apps Routing\r\n\r\nThe initial `staticwebapp.config.json` had wildcard rewrite rules that intercepted requests:\r\n\r\n```json\r\n// BEFORE - This broke pre-rendered pages\r\n{\r\n  "route": "/blog/*",\r\n  "rewrite": "/index.html"\r\n}\r\n```\r\n\r\nThis meant requests to `/blog` were being rewritten to serve the root SPA shell instead of the pre-rendered `/blog/index.html` file. Crawlers never saw the actual content.\r\n\r\n**The fix:** Remove the wildcard rewrites and let Azure serve the static files:\r\n\r\n```json\r\n// AFTER - Let static files be served naturally\r\n{\r\n  "navigationFallback": {\r\n    "rewrite": "/index.html",\r\n    "exclude": ["/assets/*", "/img/*", "/*.{json,xml,txt,ico,png,jpg,svg}"]\r\n  }\r\n}\r\n```\r\n\r\nNow the `navigationFallback` only triggers for actual 404s, while pre-rendered HTML files are served directly.\r\n\r\n## Build Tracking: Knowing What\'s in Production\r\n\r\n### The Deployment Visibility Problem\r\n\r\nWhen you\'re testing a staging site versus production, you need to know exactly which build you\'re looking at. Version numbers in `package.json` don\'t help—they don\'t auto-increment on every deployment.\r\n\r\n### Auto-Incrementing Build Versions\r\n\r\nI created a build info system that:\r\n\r\n1. **Generates a version on every build** with timestamp\r\n2. **Auto-increments** the build number\r\n3. **Displays in the footer** on every page\r\n\r\n```javascript\r\n// src/scripts/generate-build-info.mjs\r\nlet buildInfo = { version: 0, buildTime: null };\r\n\r\nif (fs.existsSync(buildInfoPath)) {\r\n  buildInfo = JSON.parse(fs.readFileSync(buildInfoPath, \'utf8\'));\r\n}\r\n\r\nbuildInfo.version = (buildInfo.version || 0) + 1;\r\nbuildInfo.buildTime = new Date().toISOString();\r\n\r\nfs.writeFileSync(buildInfoPath, JSON.stringify(buildInfo, null, 2));\r\n```\r\n\r\nThe footer now shows:\r\n\r\n```\r\n© 2026 Mark Hazleton. Built with curiosity and caffeine.\r\nBuild v5 • Jan 20, 2026, 6:23 PM CST\r\n```\r\n\r\nThis solves multiple problems:\r\n- Confirms deployments are live\r\n- Identifies which build is in production vs. staging\r\n- Provides audit trail for debugging\r\n- Validates cache invalidation\r\n\r\n## Domain Migration Strategy\r\n\r\n### The Scenario\r\n\r\n- **Old site:** Running on Azure Static Web App A with custom domain markhazleton.com\r\n- **New site:** Built and tested on Azure Static Web App B (staging URL)\r\n- **Goal:** Switch the production domain to the new site with zero downtime\r\n\r\n### Safe Migration Steps\r\n\r\n**Phase 1: Preparation**\r\n1. Verify DNS TXT record for domain validation is present\r\n2. Test new site thoroughly on staging URL\r\n3. Run Screaming Frog crawl to verify SEO\r\n4. Check build version in footer to confirm latest deployment\r\n\r\n**Phase 2: Domain Transfer**\r\n1. Remove custom domain from OLD Static Web App in Azure Portal\r\n2. Immediately add custom domain to NEW Static Web App\r\n3. Azure provides DNS validation requirements\r\n4. Update DNS records if needed (usually CNAME stays the same)\r\n5. Azure auto-provisions SSL certificate\r\n\r\n**Phase 3: Validation**\r\n1. Wait for DNS propagation (5-30 minutes typically)\r\n2. Visit https://markhazleton.com\r\n3. Check footer build version to confirm new site\r\n4. Run Screaming Frog against production domain\r\n5. Monitor for any 404s or issues\r\n\r\n**Timeline:** Total switchover typically 30-45 minutes including DNS propagation.\r\n\r\n## Exposing Data APIs\r\n\r\nThe new site needed to expose the same JSON data files at the root level for external consumption, I have a few applications that needed these routes to work:\r\n\r\n- `articles.json` - All blog post metadata\r\n- `projects.json` - All project information\r\n- `repositories.json` - GitHub activity data\r\n\r\nThese files exist in `/src/data/` during development, so I updated the publish script:\r\n\r\n```javascript\r\n// Copy data files to docs root for public API access\r\nconst dataFiles = [\'articles.json\', \'projects.json\', \'repositories.json\'];\r\n\r\nfor (const file of dataFiles) {\r\n  const sourcePath = path.join(rootDir, \'src/data\', file);\r\n  const destPath = path.join(docsDir, file);\r\n  \r\n  if (fs.existsSync(sourcePath)) {\r\n    fs.copyFileSync(sourcePath, destPath);\r\n    console.log(`Copied ${file} to /docs root`);\r\n  } else if (fs.existsSync(path.join(docsDir, \'data\', file))) {\r\n    fs.copyFileSync(\r\n      path.join(docsDir, \'data\', file),\r\n      destPath\r\n    );\r\n  }\r\n}\r\n```\r\n\r\nNow external tools can access:\r\n- `https://markhazleton.com/articles.json`\r\n- `https://markhazleton.com/projects.json`\r\n- `https://markhazleton.com/repositories.json`\r\n\r\n## Key Lessons Learned\r\n\r\n### 1. Test with Real SEO Tools\r\n\r\nGoogle\'s documentation says pre-rendered HTML is enough for SEO. But real crawlers (like Screaming Frog) sometimes struggle with minified output. Test with actual tools, not assumptions.\r\n\r\n### 2. Always Provide Fallbacks\r\n\r\nThe hidden navigation fallback seems like overkill—until it isn\'t. It\'s invisible to users, helps crawlers that struggle with complex HTML, and takes minutes to implement.\r\n\r\n### 3. Make Deployments Verifiable\r\n\r\nBuild numbers in the footer solved countless "Is this the latest version?" questions. It\'s visible to everyone, updates automatically, and provides clear audit trails.\r\n\r\n### 4. Routing Configuration Matters\r\n\r\nAzure Static Web Apps routing rules can override your carefully pre-rendered content. Understand the difference between rewrites (which replace content) and redirects (which change URLs).\r\n\r\n### 5. Domain Migration Is Low-Risk\r\n\r\nSwitching custom domains between Azure Static Web Apps is straightforward:\r\n- Remove from old\r\n- Add to new  \r\n- Wait for DNS\r\n- Verify with build version\r\n\r\nThe process is designed for zero-downtime migrations.\r\n\r\n## The Technical Stack\r\n\r\n**Frontend**\r\n- React 18 with TypeScript\r\n- Vite for build tooling\r\n- React Router for routing\r\n- Tailwind CSS for styling\r\n- shadcn/ui component library\r\n\r\n**Build Process**\r\n- Custom pre-render script for static HTML generation\r\n- Automated SEO asset generation (sitemap.xml, robots.txt, feed.xml)\r\n- Build info tracking with auto-increment versioning\r\n- GitHub Actions for CI/CD\r\n\r\n**Hosting**\r\n- Azure Static Web Apps\r\n- Custom domain with auto SSL\r\n- Global CDN distribution\r\n- Automatic deployments from GitHub\r\n\r\n**SEO Infrastructure**\r\n- Pre-rendered HTML for all routes\r\n- XML sitemap with all pages\r\n- RSS feed for blog posts\r\n- Crawler-friendly fallback navigation\r\n- HTML sitemap page\r\n- Structured data (JSON-LD)\r\n- Open Graph and Twitter Cards\r\n\r\n## Validation Results\r\n\r\nAfter implementing all fixes:\r\n\r\n**Screaming Frog Results**\r\n- ✅ Homepage shows all navigation links\r\n- ✅ Successfully crawls entire site structure  \r\n- ✅ Discovers all blog posts and projects\r\n- ✅ Validates all internal links\r\n- ✅ Hits 500 URL limit (free version) confirming full crawl\r\n\r\n**Build Tracking**\r\n- ✅ Footer shows current build version\r\n- ✅ Confirms deployment is live\r\n- ✅ Easy to verify staging vs. production\r\n\r\n**Performance**\r\n- ✅ All pages pre-rendered as static HTML\r\n- ✅ Sub-second page loads globally\r\n- ✅ Perfect Lighthouse scores possible\r\n\r\n## Conclusion: Modern Doesn\'t Mean Invisible\r\n\r\nBuilding a modern, fast, React-based site is easier than ever with tools like Vite and Azure Static Web Apps. But modern architecture creates new challenges:\r\n\r\n- Pre-rendered HTML doesn\'t guarantee crawler compatibility\r\n- Minified output can break link extraction\r\n- Routing configuration can override static files\r\n- You need explicit validation of what\'s in production\r\n\r\nThe solutions aren\'t complex:\r\n- Simple HTML fallbacks for crawler compatibility\r\n- User-friendly HTML sitemaps\r\n- Correct hosting configuration\r\n- Build version tracking\r\n\r\nBefore going live with any static site:\r\n1. Test with actual SEO tools (Screaming Frog, Google Search Console)\r\n2. Verify the homepage shows outlinks\r\n3. Check that all major pages are discoverable\r\n4. Implement build tracking for deployment verification\r\n5. Have a clear domain migration plan\r\n\r\nThe goal isn\'t just a site that works—it\'s a site that works for everyone: users, search engines, and your future self when debugging production issues.\r\n\r\n---\r\n\r\n*The new MarkHazleton.com is live, fully crawlable, and tracked with build v5. You can verify the implementation by viewing the page source and checking the footer. Everything in this article is running in production right now.*\r\n';export{e as default};

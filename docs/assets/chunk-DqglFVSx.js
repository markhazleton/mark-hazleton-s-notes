const e='---\r\nid: 49\r\nSection: Data Science\r\nslug: articles/exploratory-data-analysis-eda-using-python.html\r\nname: Exploratory Data Analysis with Python\r\ndescription: Explore essential techniques of Exploratory Data Analysis (EDA) using Python, focusing on data sanity checks and visualization methods.\r\nkeywords: Exploratory Data Analysis, Python, data visualization, data science, Mark Hazleton, EDA techniques\r\nimg_src: /img/ScotlandHighlands.jpg\r\nlastmod: 2024-06-23\r\npublishedDate: 2024-10-06\r\nestimatedReadTime: 5\r\nchangefreq: monthly\r\nsubtitle: Master the art of data exploration and visualization with Python\'s powerful libraries.\r\nauthor: Mark Hazleton\r\nsummary: Exploratory Data Analysis (EDA) is a crucial step in the data science process, allowing analysts to uncover patterns, spot anomalies, and test hypotheses. This guide delves into the techniques and tools used in EDA, with a focus on Python\'s capabilities.\r\nconclusionTitle: Key Takeaways from EDA with Python\r\nconclusionSummary: EDA is a foundational step in data analysis, offering insights and guiding further analysis. Python\'s libraries provide powerful tools for effective data exploration.\r\nconclusionKeyHeading: Bottom Line\r\nconclusionKeyText: Mastering EDA with Python empowers data scientists to make data-driven decisions confidently.\r\nconclusionText: As you continue your journey in data science, remember that EDA is not just a preliminary step but a continuous process of discovery. Utilize Python\'s tools to enhance your analytical capabilities and drive impactful insights.\r\nseo:\r\n  title: Exploratory Data Analysis with Python \r\n  titleSuffix:  \r\n  description: Discover essential techniques for Exploratory Data Analysis using Python. Learn data sanity checks and visualization methods to enhance your data insights.\r\n  keywords: Exploratory Data Analysis, Python, EDA, data visualization, data science, Mark Hazleton\r\n  canonical: https://markhazleton.com/articles/exploratory-data-analysis-eda-using-python.html\r\n  robots: index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1\r\nog:\r\n  title: Exploratory Data Analysis with Python\r\n  description: Discover essential techniques for Exploratory Data Analysis using Python. Learn data sanity checks and visualization methods to enhance your data insights.\r\n  type: article\r\n  image: null\r\n  imageAlt: Exploratory Data Analysis (EDA) Using Python - Mark Hazleton\r\ntwitter:\r\n  title: EDA with Python\r\n  description: Discover essential techniques for Exploratory Data Analysis using Python. Learn data sanity checks and visualization methods to enhance your data insights.\r\n  image: null\r\n  imageAlt: Exploratory Data Analysis (EDA) Using Python - Mark Hazleton\r\nyoutubeUrl: https://www.youtube.com/watch?v=3KO0GuTkPew\r\nyoutubeTitle: "Deep Dive:  EDA with Python"\r\n---\r\n\r\n# Exploratory Data Analysis (EDA) Using Python\r\n\r\n## A Comprehensive Guide to Data Sanity Checks and EDA\r\n\r\nBefore venturing into advanced data analysis or machine learning, it\'s essential to ensure that the data you\'re working with is clean and coherent. This article outlines the process of conducting Data Sanity checks and Exploratory Data Analysis (EDA), both of which are critical first steps in understanding your dataset.\r\n\r\nWhile the initial stages don\'t involve modifying the data, these actions help uncover potential issues such as missing values, duplicates, and outliers, while providing valuable insights into the data\'s structure and relationships.\r\n\r\nIt\'s important to start by inspecting the dataset to familiarize yourself with its contents and structure. Understanding the data is key to identifying any problems that might affect your analysis. Once this preliminary examination is complete, you can decide whether data cleaning or preprocessing is needed.\r\n\r\n### Key Focus Areas\r\n\r\n- Data sanity checks\r\n- Exploratory data analysis\r\n- Missing value detection\r\n- Outlier identification\r\n\r\n## Table of Contents\r\n\r\n1. [Understanding the Dataset](#understanding-the-dataset)\r\n2. [Data Sanity Checks](#data-sanity-checks)\r\n3. [Exploratory Data Analysis (EDA)](#exploratory-data-analysis-eda)\r\n4. [Automating Univariate and Bivariate Analysis](#automating-univariate-and-bivariate-analysis)\r\n5. [Plan for Data Cleaning and Preprocessing](#plan-for-data-cleaning-and-preprocessing)\r\n6. [Conclusion](#conclusion)\r\n7. [EDA FAQ](#eda-faq)\r\n8. [Summary Checklist](#summary-checklist)\r\n9. [Glossary](#glossary)\r\n\r\n## Understanding the Dataset\r\n\r\nRecently, I worked with a nutritional dataset from Kaggle using Google Colab, which allows for writing and executing Python code in a browser-based Jupyter notebook. The objective was to analyze nutrient patterns across a range of foods and categorize them based on their nutrient content.\r\n\r\nKaggle is a fantastic resource for data scientists and enthusiasts, offering a wealth of datasets across different fields. Whether you\'re looking to explore healthcare, finance, or more specialized areas like nutrition, Kaggle provides an excellent platform to practice data analysis and machine learning techniques.\r\n\r\n**Dataset Details:** The dataset I explored included nutritional values for various foods and products, detailing their protein, fat, vitamin C, and fiber content. You can find the full code and analysis in my [Google Drive folder](https://drive.google.com/drive/folders/1cF49bLIgTwHMNwo7TxSjMw_8m-yw2cBg?usp=sharing).\r\n\r\n## Data Sanity Checks\r\n\r\nBefore diving into EDA, it\'s crucial to perform data sanity checks to ensure the dataset\'s integrity:\r\n\r\n- **Check for Missing Values:** Identify and handle missing data appropriately.\r\n- **Check for Duplicates:** Remove duplicate entries to avoid skewed results.\r\n- **Data Types Verification:** Ensure data types are correct for each column.\r\n- **Range Validation:** Verify that numerical values fall within expected ranges.\r\n- **Consistency Checks:** Ensure data is consistent across related fields.\r\n\r\n## Exploratory Data Analysis (EDA)\r\n\r\nExploratory Data Analysis (EDA) is a method used by data scientists to analyze datasets and summarize their main characteristics, often using visual methods. It is a critical step in understanding the data before proceeding with more complex analyses or modeling.\r\n\r\n### Why EDA?\r\n\r\n- **Identify Patterns:** EDA helps in identifying patterns and relationships in data.\r\n- **Spot Anomalies:** It allows for the detection of outliers and anomalies that might skew the analysis.\r\n- **Hypothesis Testing:** EDA provides a foundation for hypothesis testing and further statistical analysis.\r\n\r\n### Tools and Libraries\r\n\r\nPython offers several libraries that are essential for conducting EDA:\r\n\r\n- **Pandas:** For data manipulation and analysis.\r\n- **Matplotlib:** For creating static, interactive, and animated visualizations.\r\n- **Seaborn:** Built on top of Matplotlib, it provides a high-level interface for drawing attractive statistical graphics.\r\n- **NumPy:** For numerical data processing.\r\n\r\n### Visualization Techniques\r\n\r\n- **Histograms:** To understand the distribution of a single variable.\r\n- **Scatter Plots:** To identify relationships between two variables.\r\n- **Box Plots:** To visualize the spread and identify outliers.\r\n- **Heatmaps:** To display the correlation between variables.\r\n\r\n## Automating Univariate and Bivariate Analysis\r\n\r\nDuring your Data Sanity checks, it\'s essential to classify your variables into numerical, categorical, and dependent types before starting your Exploratory Data Analysis (EDA).\r\n\r\nIn the early stages of data analysis, you will often need to determine whether your variables are numerical, categorical, or dependent. Identifying these is crucial for:\r\n\r\n- Performing the correct statistical methods on your data\r\n- Automating your exploratory analysis using scripts\r\n- Generating meaningful insights into relationships between features\r\n\r\nOnce these variables are classified, you can begin the process of performing univariate (analyzing one variable) and bivariate (analyzing relationships between two variables) analysis. Automating this process will save you time and ensure consistency in your Exploratory Data Analysis (EDA).\r\n\r\n### Numerical Univariate Analysis\r\n\r\nThis function calculates the key statistical attributes for a numerical feature, including mean, median, variance, and skewness. It also provides visual insights using KDE plots, BoxPlots, and Histograms.\r\n\r\n```python\r\ndef univariate_analysis(df, features):\r\n    for feature in features:\r\n        skewness = df[feature].skew()\r\n        minimum = df[feature].min()\r\n        maximum = df[feature].max()\r\n        mean = df[feature].mean()\r\n        mode = df[feature].mode().values[0]\r\n        unique_count = df[feature].nunique()\r\n        variance = df[feature].var()\r\n        std_dev = df[feature].std()\r\n        percentile_25 = df[feature].quantile(0.25)\r\n        median = df[feature].median()\r\n        percentile_75 = df[feature].quantile(0.75)\r\n        data_range = maximum - minimum\r\n\r\n        print(f"Univariate Analysis for {feature}")\r\n        print(f"Skewness: {skewness:.4f}")\r\n        print(f"Min: {minimum}")\r\n        print(f"Max: {maximum}")\r\n        print(f"Mean: {mean:.4f}")\r\n        print(f"Mode: {mode}")\r\n        print(f"Unique Count: {unique_count}")\r\n        print(f"Variance: {variance:.4f}")\r\n        print(f"Std Dev: {std_dev:.4f}")\r\n        print(f"25th Percentile: {percentile_25}")\r\n        print(f"Median (50th Pct): {median}")\r\n        print(f"75th Percentile: {percentile_75}")\r\n        print(f"Range: {data_range}")\r\n\r\n        plt.figure(figsize=(18, 6))\r\n        plt.subplot(1, 3, 1)\r\n        sns.kdeplot(df[feature], fill=True)\r\n        plt.title(f"KDE of {feature}")\r\n        plt.subplot(1, 3, 2)\r\n        sns.boxplot(df[feature])\r\n        plt.title(f"Box Plot of {feature}")\r\n        plt.subplot(1, 3, 3)\r\n        sns.histplot(df[feature], bins=10, kde=True)\r\n        plt.title(f"Histogram of {feature}")\r\n        plt.tight_layout()\r\n        plt.show()\r\n```\r\n\r\nThis function provides a comprehensive analysis for each numerical feature by calculating statistical attributes and generating KDE, BoxPlot, and Histogram visualizations.\r\n\r\n### Categorical Univariate Analysis\r\n\r\nFor categorical features, we analyze the distribution of categories and their relationship with the dependent feature. Here\'s a function that automates this process:\r\n\r\n```python\r\ndef univariate_analysis_categorical(df, categorical_features):\r\n    for feature in categorical_features:\r\n        unique_categories = df[feature].nunique()\r\n        mode = df[feature].mode().values[0]\r\n        mode_freq = df[feature].value_counts().max()\r\n        category_counts = df[feature].value_counts()\r\n        category_percent = df[feature].value_counts(normalize=True) * 100\r\n        missing_values = df[feature].isnull().sum()\r\n        total_values = len(df[feature])\r\n        imbalance_ratio = category_counts.max() / total_values\r\n\r\n        print(f"Univariate Analysis for {feature}")\r\n        print(f"Unique Categories: {unique_categories}")\r\n        print(f"Mode (Most frequent): {mode}")\r\n        print(f"Frequency of Mode: {mode_freq}")\r\n        print(f"Missing Values: {missing_values}")\r\n        print(f"Imbalance Ratio (Max/Total): {imbalance_ratio:.4f}")\r\n        print(f"Category Counts:\\n{category_counts}")\r\n\r\n        plt.figure(figsize=(10, 6))\r\n        sns.countplot(x=df[feature], order=df[feature].value_counts().index)\r\n        plt.title(f"Frequency of {feature} Categories")\r\n        plt.xlabel(feature)\r\n        plt.ylabel("Count")\r\n        plt.tight_layout()\r\n        plt.show()\r\n```\r\n\r\nThis function provides a clear understanding of how categories are distributed across the data and helps identify potential imbalances.\r\n\r\n### Automating Bivariate Analysis\r\n\r\nBivariate analysis allows you to understand the relationship between two variables. The following function calculates key attributes for a numerical feature in relation to a boolean dependent feature. It prints out key insights and generates side-by-side visualizations to understand their relationship.\r\n\r\n```python\r\ndef bivariate_analysis(df, numerical_features, categorical_features, dependent_feature):\r\n    if numerical_features:\r\n        for feature in numerical_features:\r\n            mean_0 = df[df[dependent_feature] == 0][feature].mean()\r\n            mean_1 = df[df[dependent_feature] == 1][feature].mean()\r\n            median_0 = df[df[dependent_feature] == 0][feature].median()\r\n            median_1 = df[df[dependent_feature] == 1][feature].median()\r\n            var_0 = df[df[dependent_feature] == 0][feature].var()\r\n            var_1 = df[df[dependent_feature] == 1][feature].var()\r\n\r\n            print(f"Mean {feature} for group 0: {mean_0:.2f}")\r\n            print(f"Mean {feature} for group 1: {mean_1:.2f}")\r\n            print(f"Median {feature} for group 0: {median_0:.2f}")\r\n            print(f"Median {feature} for group 1: {median_1:.2f}")\r\n            print(f"Variance of {feature} for group 0: {var_0:.2f}")\r\n            print(f"Variance of {feature} for group 1: {var_1:.2f}")\r\n\r\n            fig, axes = plt.subplots(1, 2, figsize=(16, 6))\r\n            sns.boxplot(x=df[dependent_feature], y=df[feature], ax=axes[0])\r\n            axes[0].set_title(f"{feature} Distribution by {dependent_feature}")\r\n            sns.barplot(x=df[dependent_feature], y=df[feature], estimator=\'mean\', ax=axes[1])\r\n            axes[1].set_title(f"Mean {feature} by {dependent_feature}")\r\n            plt.tight_layout()\r\n            plt.show()\r\n\r\n    if categorical_features:\r\n        for feature in categorical_features:\r\n            category_distribution = df.groupby([feature, dependent_feature]).size().unstack(fill_value=0)\r\n            chi2, p, dof, expected = chi2_contingency(category_distribution)\r\n\r\n            print(f"Chi-Square Test for {feature}: Chi2 = {chi2:.4f}, p-value = {p:.4f}")\r\n            fig, axes = plt.subplots(1, 2, figsize=(16, 6))\r\n            sns.countplot(x=df[feature], hue=df[dependent_feature], ax=axes[0])\r\n            axes[0].set_title(f"{feature} Count by {dependent_feature}")\r\n            sns.barplot(x=df[feature], y=df[dependent_feature], estimator=\'mean\', ax=axes[1])\r\n            axes[1].set_title(f"Proportion of {dependent_feature} by {feature}")\r\n            plt.tight_layout()\r\n            plt.show()\r\n```\r\n\r\nThis function performs bivariate analysis by calculating key attributes and generating box plots, bar plots, and count plots to help you better understand the relationship between variables.\r\n\r\n## Plan for Data Cleaning and Preprocessing\r\n\r\nBy conducting thorough data sanity checks and EDA, we lay a strong foundation for further analysis. With a clear understanding of the data, the next steps could include feature engineering, advanced visualizations, or machine learning.\r\n\r\n## Conclusion\r\n\r\nExploratory Data Analysis is an indispensable part of the data analysis process. By leveraging Python\'s robust libraries, analysts can gain deep insights into their data, paving the way for more informed decision-making.\r\n\r\nAs you continue your journey in data science, remember that EDA is not just a preliminary step but a continuous process of discovery. Utilize Python\'s tools to enhance your analytical capabilities and drive impactful insights.\r\n\r\n### Key Takeaways from EDA with Python\r\n\r\n- EDA is a foundational step in data analysis, offering insights and guiding further analysis.\r\n- Python\'s libraries provide powerful tools for effective data exploration.\r\n- Mastering EDA with Python empowers data scientists to make data-driven decisions confidently.\r\n\r\n## EDA FAQ\r\n\r\n### What is Exploratory Data Analysis (EDA)?\r\n\r\nEDA is the process of analyzing datasets to summarize their main characteristics, often using visual methods.\r\n\r\n### Why is EDA important?\r\n\r\nEDA helps you understand your data, detect anomalies, test assumptions, and prepare for modeling.\r\n\r\n### What are common EDA techniques?\r\n\r\nCommon techniques include summary statistics, visualizations (histograms, boxplots, scatter plots), and correlation analysis.\r\n\r\n### How do I handle missing data in EDA?\r\n\r\nIdentify missing values, then decide whether to remove, impute, or flag them based on context.\r\n\r\n### What Python libraries are best for EDA?\r\n\r\npandas, matplotlib, seaborn, and missingno are popular libraries for EDA in Python.\r\n\r\n## Summary Checklist\r\n\r\n- ✅ Data sanity checks and EDA steps explained\r\n- ✅ Complete Python code samples for automated analysis\r\n- ✅ Univariate and bivariate analysis functions provided\r\n- ✅ Visual techniques and statistical methods covered\r\n- ✅ Table of Contents, FAQ, and Glossary included\r\n- ✅ Real-world dataset example with Google Colab workflow\r\n\r\n## Glossary\r\n\r\n### EDA (Exploratory Data Analysis)\r\n\r\nExploratory Data Analysis (EDA) is the process of analyzing datasets to summarize their main characteristics, often using visual methods.\r\n\r\nFor more, see the [Wikipedia article on EDA](https://en.wikipedia.org/wiki/Exploratory_data_analysis).\r\n\r\n### Outlier\r\n\r\nAn outlier is a data point that differs significantly from other observations in a dataset.\r\n\r\nLearn more at [Wikipedia: Outlier](https://en.wikipedia.org/wiki/Outlier).\r\n\r\n### IQR (Interquartile Range)\r\n\r\nThe interquartile range (IQR) is a measure of statistical dispersion, being equal to the difference between the upper and lower quartiles.\r\n\r\nSee [Wikipedia: Interquartile range](https://en.wikipedia.org/wiki/Interquartile_range) for details.\r\n\r\n### Skewness\r\n\r\nSkewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean.\r\n\r\nMore info: [Wikipedia: Skewness](https://en.wikipedia.org/wiki/Skewness).\r\n\r\n### Pandas\r\n\r\nPandas is a powerful open-source Python library for data manipulation and analysis, providing flexible data structures like DataFrames.\r\n\r\nSee [Wikipedia: Pandas (software)](https://en.wikipedia.org/wiki/Pandas_(software)).\r\n\r\n## Explore More Data Science Articles\r\n\r\nDive deeper into data science topics:\r\n\r\n- [Python: The Language of Data Science](/articles/python-the-language-of-data-science.html)\r\n- [An Introduction to Neural Networks](/articles/an-introduction-to-neural-networks.html)\r\n';export{e as default};

const n='# Building a Quick Estimation Template When You Have Almost Nothing to Go On\n\n> When faced with vague requirements and tight deadlines, I built a simple three-pillar framework using Innovation, Scope, and People for estimating quickly.\n\nCategory: Project Management\n\n---\n\n## Why You Need a Quick Estimation Template\n\nThere are moments in project management when leadership asks, “How long will this take?” and all you have is a one-liner and a deadline. Requirements are nebulous, resources are unclear, and the risks are unknown—yet you still need a number. In these situations, you don’t need a perfect plan; you need a credible, defensible, and quick estimate that communicates uncertainty honestly.\n\nThis article presents a pragmatic, repeatable approach built around a three-pillar model—Innovation, Scope, and People—to deliver fast estimates with traceable logic. It helps you move from “I have almost nothing” to “Here’s a 50/70/90 estimate with assumptions,” in under an hour.\n\n---\n\n## Principles of Estimating in the Fog\n\n- Fast over perfect: Provide a bounded, defensible range within 30–60 minutes.\n- Honest uncertainty: Communicate confidence levels and assumptions up front.\n- Repeatable structure: Use a compact template you can refine over time.\n- Calibrate as you learn: Track real vs. estimated to tighten multipliers.\n- Visible trade-offs: Show how adding information reduces uncertainty.\n\nFor context, see the Cone of Uncertainty, which shows how estimates become more accurate as knowledge increases: https://en.wikipedia.org/wiki/Cone_of_Uncertainty\n\n---\n\n## The Three-Pillar Framework\n\nAt low fidelity, everything collapses into three drivers:\n\n1) Innovation (I): How novel is this work?\n2) Scope (S): How much work do we think is included?\n3) People (P): Who’s doing it, and how are they organized?\n\nEach pillar is scored quickly on a 1–5 scale. Scores map to multipliers that inflate or deflate the base estimate. This keeps the math simple and the reasoning explainable.\n\n### Pillar 1: Innovation (Novelty and Uncertainty)\n\n- 1 – Purely routine work with playbooks\n- 2 – Minor variations on familiar patterns\n- 3 – Some unknowns; expected trial-and-error\n- 4 – High novelty; integration with unfamiliar tech\n- 5 – R&D-like; unclear feasibility\n\nWhat to ask:\n- Is there precedent internally or externally?\n- Are there new technologies, vendors, or APIs?\n- Are there unknown performance/security/stability constraints?\n- Will we prototype or spike to learn?\n\n### Pillar 2: Scope (Breadth and Depth)\n\n- 1 – Single small deliverable\n- 2 – A few related deliverables with limited integration\n- 3 – Moderate set of deliverables with some cross-team dependencies\n- 4 – Complex feature set; multi-system integrations\n- 5 – Program-level scope; many moving parts\n\nWhat to ask:\n- What’s in vs. out of scope (even roughly)?\n- How many components, integrations, or environments?\n- What are the dependencies or non-functional requirements?\n- What does “done” mean?\n\n### Pillar 3: People (Capability and Configuration)\n\n- 1 – Expert team with proven track record in this domain\n- 2 – Strong team, one or two gaps\n- 3 – Capable team; limited domain experience; some context switching\n- 4 – Mixed capabilities; changing priorities; partial availability\n- 5 – New team; low availability; external coordination\n\nWhat to ask:\n- Who is available and for how many hours per week?\n- Do we have domain expertise?\n- Are roles covered (e.g., engineering, QA, design, PM)?\n- Are decision-makers accessible?\n\n---\n\n## The Scoring Matrix and Multipliers\n\nUse scores to select multipliers. These are starting points; calibrate them to your org.\n\n| Pillar     | Score | Multiplier | Heuristic description |\n|------------|-------|------------|-----------------------|\n| Innovation | 1     | 0.90       | Routine, low uncertainty |\n|            | 2     | 1.00       | Familiar work          |\n|            | 3     | 1.20       | Some unknowns          |\n|            | 4     | 1.50       | High novelty           |\n|            | 5     | 2.00       | R&D/prototype          |\n| Scope      | 1     | 0.80       | Very limited           |\n|            | 2     | 1.00       | Small-to-medium        |\n|            | 3     | 1.30       | Moderate complexity    |\n|            | 4     | 1.60       | Complex integrations   |\n|            | 5     | 2.20       | Program-level          |\n| People     | 1     | 0.85       | Elite, stable team     |\n|            | 2     | 1.00       | Strong coverage        |\n|            | 3     | 1.20       | Gaps or context switching |\n|            | 4     | 1.50       | Availability issues    |\n|            | 5     | 1.90       | New team / external    |\n\nNotes:\n- Innovation and Scope usually inflate the estimate as they rise.\n- People often inflates when less optimal (higher score = larger multiplier).\n- Multipliers compound: TotalMultiplier = I × S × P.\n\n---\n\n## A Minimal Estimation Formula\n\n- Start with a base scope size in abstract units (e.g., story points or T-shirt sizes converted).\n- Translate into person-days using a baseline throughput.\n- Apply multipliers to account for innovation, scope uncertainty, and team factors.\n- Add a contingency consistent with confidence levels.\n\nSuggested defaults:\n- Baseline throughput: 1 story point ≈ 0.75 person-days (adjust per team)\n- Alternatively: 1 small feature ≈ 3–5 days; 1 integration ≈ 5–10 days\n\nFormula:\n- Base person-days = ScopeUnits × Throughput\n- Adjusted person-days = Base × I × S × P\n- 50% estimate (P50) = Adjusted\n- 70% estimate (P70) = P50 × 1.2\n- 90% estimate (P90) = P50 × 1.5\n\nThese P50/P70/P90 factors are simple proxies when you can’t run full risk modeling. Replace with your own calibrated ratios over time.\n\n---\n\n## Quick Start: 15-Minute Estimation Interview\n\nAsk:\n- What is the primary outcome? What does “done” look like?\n- What’s in/out? Name three things definitely not included.\n- Who is available? Any hard capacity limits?\n- Which systems are involved? Any new vendors or tech?\n- What date is driving this? What is flexible?\n\nThen:\n- Assign I/S/P scores.\n- Choose a rough scope unit count (e.g., 8–15 points).\n- Compute P50/P70/P90.\n\n---\n\n## The One-Page Estimation Template (Markdown)\n\nCopy/paste this into your ticket, doc, or email.\n\n```\n# Quick Estimate — <Project/Feature Name>\nDate: <YYYY-MM-DD>\nEstimator: <Name>\nConfidence: P50/P70/P90\n\nOutcome (one-liner):\n- <Describe the measurable outcome or deliverable>\n\nAssumptions:\n- <List key assumptions>\n- <What’s explicitly out-of-scope>\n\nThree-Pillar Scores:\n- Innovation (I): <1–5>  → Multiplier: <X.XX>\n- Scope (S): <1–5>       → Multiplier: <X.XX>\n- People (P): <1–5>      → Multiplier: <X.XX>\n\nScope Size:\n- Units: <story points / features / tasks>\n- Quantity: <N>\n- Throughput: <units-to-days conversion>\n\nMath:\n- Base person-days = <N × throughput>\n- Adjusted = Base × I × S × P\n- P50 = <Adjusted>\n- P70 = <Adjusted × 1.2>\n- P90 = <Adjusted × 1.5>\n\nRisks & Unknowns:\n- <Top 3–5 risks>\n- <Mitigations or spikes>\n\nDependencies:\n- <Teams, vendors, approvals>\n\nDecision/Trade-offs:\n- If we drop X, we save ~Y days\n- If we defer Z, risk reduces by ~R%\n\nNext Steps (to refine estimate):\n- <Spike A> (1–2 days) to validate <unknown>\n- <Stakeholder review> to confirm scope\n```\n\n---\n\n## JSON/YAML Template for Tooling\n\nIf you use scripts or dashboards, you can capture inputs like this:\n\n```json\n{\n  "project": "New Analytics Dashboard",\n  "date": "2025-01-15",\n  "scope_units": 14,\n  "throughput_days_per_unit": 0.8,\n  "innovation_score": 3,\n  "scope_score": 4,\n  "people_score": 2,\n  "multipliers": {\n    "innovation": { "1": 0.9, "2": 1.0, "3": 1.2, "4": 1.5, "5": 2.0 },\n    "scope":      { "1": 0.8, "2": 1.0, "3": 1.3, "4": 1.6, "5": 2.2 },\n    "people":     { "1": 0.85, "2": 1.0, "3": 1.2, "4": 1.5, "5": 1.9 }\n  },\n  "confidence_factors": { "p70": 1.2, "p90": 1.5 },\n  "assumptions": [\n    "Single data warehouse source",\n    "Two chart types at launch",\n    "No SSO integration in v1"\n  ]\n}\n```\n\n```yaml\nproject: New Analytics Dashboard\ndate: 2025-01-15\nscope_units: 14\nthroughput_days_per_unit: 0.8\ninnovation_score: 3\nscope_score: 4\npeople_score: 2\nconfidence_factors:\n  p70: 1.2\n  p90: 1.5\nassumptions:\n  - Single data warehouse source\n  - Two chart types at launch\n  - No SSO integration in v1\n```\n\n---\n\n## Example Walkthrough\n\nScenario:\n- Outcome: MVP of a customer-facing dashboard with filtering and export.\n- Constraints: Demo in 6 weeks.\n- Team: One senior engineer (70%), one mid-level (50%), shared designer (25%).\n- Risks: Unknown export format standard; new BI library.\n\nScores:\n- Innovation (I) = 3 (some unknowns with BI library)\n- Scope (S) = 4 (integrations with auth, data, export)\n- People (P) = 3 (partial availability, mixed levels)\n\nScope and throughput:\n- 16 story points at 0.75 days/point\n- Base = 16 × 0.75 = 12 person-days\n\nMultipliers:\n- I=1.2, S=1.6, P=1.2 → Total = 1.2 × 1.6 × 1.2 = 2.304\n\nAdjusted:\n- P50 = 12 × 2.304 = 27.648 ≈ 28 person-days\n- P70 = 28 × 1.2 = 33.6 ≈ 34 person-days\n- P90 = 28 × 1.5 = 42 person-days\n\nCalendar implications:\n- With ~1.45 FTE (0.7 + 0.5 + 0.25×0.5 for design), say 7 person-days/week\n- P50 timeline ≈ 4 weeks, P70 ≈ 5 weeks, P90 ≈ 6 weeks\n- Conclusion: Demo feasible, but reserve scope cuts if risks materialize.\n\nTrade-offs:\n- Drop export v1 → save ~4–6 days; reduce risk\n- Replace BI library with plain charts → save ~2–3 days learning curve\n\n---\n\n## Monte Carlo Option (When You Have 10 More Minutes)\n\nUse a simple simulation to convert multipliers and scope variance into confidence ranges.\n\n```python\nimport json, random, statistics as stats\n\nconfig = {\n    "scope_units": 16,\n    "throughput_days_per_unit": 0.75,\n    "multipliers": {"I": 1.2, "S": 1.6, "P": 1.2},\n    "scope_variation": 0.25,   # ±25%\n    "mult_variation": 0.10,    # ±10% each multiplier\n    "trials": 5000\n}\n\ndef sample_uniform(center, spread):\n    return random.uniform(center*(1-spread), center*(1+spread))\n\ndef simulate(cfg):\n    base = cfg["scope_units"] * cfg["throughput_days_per_unit"]\n    samples = []\n    for _ in range(cfg["trials"]):\n        scope = sample_uniform(cfg["scope_units"], cfg["scope_variation"])\n        i = sample_uniform(cfg["multipliers"]["I"], cfg["mult_variation"])\n        s = sample_uniform(cfg["multipliers"]["S"], cfg["mult_variation"])\n        p = sample_uniform(cfg["multipliers"]["P"], cfg["mult_variation"])\n        samples.append((scope * cfg["throughput_days_per_unit"]) * i * s * p)\n    samples.sort()\n    def percentile(pct): return samples[int(len(samples)*pct)]\n    return {\n        "p50": percentile(0.50),\n        "p70": percentile(0.70),\n        "p90": percentile(0.90),\n        "mean": stats.mean(samples),\n        "stddev": stats.pstdev(samples)\n    }\n\nprint(simulate(config))\n```\n\nThis is not over-engineering; it provides a quick sanity check on your P70/P90.\n\n---\n\n## Spreadsheet-Friendly Formulas\n\n- Base person-days:\n  - = ScopeUnits × Throughput\n- Total multiplier:\n  - = I_Mult × S_Mult × P_Mult\n- P50:\n  - = Base × TotalMult\n- P70:\n  - = P50 × 1.2\n- P90:\n  - = P50 × 1.5\n\nFor Google Sheets with dropdowns and a lookup table:\n- Suppose A2=InnovationScore, B2=ScopeScore, C2=PeopleScore, and you have a lookup table in H2:J6 for multipliers. Then:\n  - I_Mult: =INDEX($H$2:$H$6, A2)\n  - S_Mult: =INDEX($I$2:$I$6, B2)\n  - P_Mult: =INDEX($J$2:$J$6, C2)\n  - Total: =PRODUCT(I_Mult, S_Mult, P_Mult)\n\n---\n\n## Risk and Contingency: Aligning with Confidence\n\nWhen nothing is certain, ranges are more honest than single numbers. Map risk appetite to buffers:\n\n| Confidence | Factor | Use when… |\n|------------|--------|-----------|\n| P50        | 1.0×   | Internal planning, low penalty for slippage |\n| P70        | 1.2×   | Stakeholder commitments with moderate risk |\n| P90        | 1.5×   | External commitments, penalties, or launches |\n\nTip:\n- Quote “P70: 5 weeks (range 4–6)” rather than “5 weeks.”\n- Pair with key assumptions; commit to updating within 3–5 business days as unknowns clarify.\n\n---\n\n## Communication Template (Email/Slack)\n\n```\nHere’s a quick estimate for <Project> based on limited info:\n\nP50: ~28 person-days\nP70: ~34 person-days\nP90: ~42 person-days\n\nAssumptions:\n- Single data source; no SSO\n- Two visualizations at launch\n- Partial team availability\n\nDrivers (multipliers):\n- Innovation=1.2 (new BI lib)\n- Scope=1.6 (multiple integrations)\n- People=1.2 (partial availability)\n\nTop risks:\n- Export format ambiguity\n- Data quality variance\n\nNext steps to reduce uncertainty (within 3 days):\n- 1-day spike to validate export format\n- Stakeholder review to confirm must-have charts\n\nIf we drop export in v1: save ~4–6 days.\n```\n\n---\n\n## Calibrating Over Time\n\nYour first multipliers are guesses. Make them better:\n\n- Track: planned (P50) vs. actuals at a task/feature level.\n- Categorize: routine vs. novel, integration-heavy vs. UI-heavy.\n- Regress: adjust multipliers and throughput quarterly.\n- Watch variance: if your P90 misses often, increase buffers.\n- Codify: publish a 1-pager of “current org multipliers.”\n\nCalibration practice:\n- Compute Actual/Base for completed items.\n- Compare by pillar scoring to see bias (e.g., people=4 often underestimates by 30%).\n- Update multiplier table accordingly.\n\n---\n\n## Context Variations\n\n- Software Delivery\n  - Throughput via historical velocity: 1 point ≈ team-days/velocity.\n  - Innovation spike tickets to de-risk libraries, API quotas, infra.\n- Data Projects\n  - Treat data quality/availability as innovation risk.\n  - Scope includes pipelines, transformations, lineage, validation.\n- Design/Research\n  - Throughput in artifacts/week; innovation includes new user segments.\n- Operations/Infrastructure\n  - People multiplier more sensitive to change windows and approvals.\n  - Scope includes environments, runbooks, rollback plans.\n\n---\n\n## Common Pitfalls and How to Avoid Them\n\n- Pitfall: Anchoring on a single number.\n  - Fix: Always present P50/P70/P90 with assumptions.\n- Pitfall: Ignoring availability and context switching.\n  - Fix: Use People multiplier and explicit FTE assumptions.\n- Pitfall: Hidden scope in non-functional requirements.\n  - Fix: Include deployment, security, observability in scope checklist.\n- Pitfall: “Unknown unknowns” hand-waving.\n  - Fix: Include a time-boxed spike to turn unknowns into knowns.\n- Pitfall: No follow-up refinement.\n  - Fix: Set a refinement checkpoint date in the estimate.\n\n---\n\n## A Lightweight Risk Checklist\n\n- Integrations: new vendor, auth, rate limits?\n- Data: quality, volume, latency, privacy?\n- Compliance: approvals, audit, change windows?\n- Performance: SLAs, load profiles?\n- Environments: dev/test/stage/prod parity, infra readiness?\n- People: key person risk, onboarding time?\n- External: dependencies on other teams’ backlogs?\n\n---\n\n## Quick Reference: T-shirt Sizing Conversion\n\nUse when tasks are non-technical or mixed-discipline.\n\n| Size | Person-days (baseline) |\n|------|------------------------|\n| XS   | 0.5–1                  |\n| S    | 1–3                    |\n| M    | 3–5                    |\n| L    | 5–8                    |\n| XL   | 8–13                   |\n\nThen apply I/S/P multipliers just as you would for points.\n\n---\n\n## Putting It All Together: A Worked Micro-Example\n\n- Feature: “Add email passwordless login.”\n- Assumptions: Use existing auth provider; mobile and web; no SSO v1.\n- Scores: I=2, S=3, P=2 → Multipliers: 1.0 × 1.3 × 1.0 = 1.3\n- Scope: 10 points; throughput 0.8 days/point → Base = 8 days\n- P50: 8 × 1.3 = 10.4 ≈ 10.5 days\n- P70: 12.6 days; P90: 15.8 days\n- Communication: “P70: ~2.5 weeks for a 1-FTE engineer; risk reduced if we reuse existing session flows.”\n\n---\n\n## Implementation Snippet: CLI Estimator\n\nFor quick terminal usage:\n\n```python\n#!/usr/bin/env python3\nimport argparse\n\nI_MULT = {1:0.9, 2:1.0, 3:1.2, 4:1.5, 5:2.0}\nS_MULT = {1:0.8, 2:1.0, 3:1.3, 4:1.6, 5:2.2}\nP_MULT = {1:0.85,2:1.0, 3:1.2, 4:1.5, 5:1.9}\n\ndef estimate(units, days_per_unit, i, s, p, p70=1.2, p90=1.5):\n    base = units * days_per_unit\n    total_mult = I_MULT[i] * S_MULT[s] * P_MULT[p]\n    p50 = base * total_mult\n    return p50, p50*p70, p50*p90\n\nif __name__ == "__main__":\n    ap = argparse.ArgumentParser()\n    ap.add_argument("--units", type=float, required=True)\n    ap.add_argument("--days_per_unit", type=float, default=0.75)\n    ap.add_argument("--i", type=int, required=True)\n    ap.add_argument("--s", type=int, required=True)\n    ap.add_argument("--p", type=int, required=True)\n    args = ap.parse_args()\n    p50, p70, p90 = estimate(args.units, args.days_per_unit, args.i, args.s, args.p)\n    print(f"P50: {p50:.1f} days | P70: {p70:.1f} | P90: {p90:.1f}")\n```\n\nUsage:\n- ./estimate.py --units 16 --days_per_unit 0.75 --i 3 --s 4 --p 2\n\n---\n\n## How to Present to Stakeholders\n\n- Lead with outcomes and ranges:\n  - “To deliver X, we estimate P70: 5 weeks (range 4–6), assuming Y.”\n- Highlight top 2–3 uncertainties and the plan to reduce them.\n- Offer scope levers: “If we drop A, we save B days.”\n- Time-box learning: “We’ll run a 2-day spike and report back by Friday.”\n- Ask for decisions: “We need sign-off on C to hold P70.”\n\n---\n\n## FAQ\n\n- Why not just use story points?\n  - Points are helpful, but in early stages you still need a conversion and a way to express uncertainty. The three-pillar multipliers make your assumptions explicit.\n- Isn’t multiplying multipliers risky?\n  - Yes, compounding can inflate quickly. That’s intentional—uncertainties multiply in real life. Calibrate to your context.\n- What about fixed-price contracts?\n  - Use P90 or higher and enumerate assumptions in the SOW. Price change orders for scope additions or assumption violations.\n- Can this work outside software?\n  - Yes. Substitute scope units with appropriate measures (deliverables, interviews, pages designed, servers configured).\n\n---\n\n## Further Reading\n\n- Cone of Uncertainty: https://en.wikipedia.org/wiki/Cone_of_Uncertainty\n- Brooks’s Law (team scaling risks): https://en.wikipedia.org/wiki/Brooks%27s_law\n- Relative estimation and Story Points: https://www.scrum.org/resources/blog/what-are-story-points\n- Monte Carlo in project management: https://en.wikipedia.org/wiki/Monte_Carlo_method\n\n---\n\n## Final Thoughts\n\nWhen you have almost nothing to go on, the goal isn’t precision—it’s clarity. A quick, transparent estimate with Innovation, Scope, and People puts structure around ambiguity, communicates risk honestly, and creates a path to reduce uncertainty. Use this template to get to a credible P50/P70/P90 in under an hour, then iterate as you learn.\n\nShip the estimate, time-box the unknowns, and refine. That’s how you deliver under uncertainty.';export{n as default};

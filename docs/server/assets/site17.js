const e='---\r\nid: 95\r\nSection: AI & Machine Learning\r\nslug: articles/building-museumspark-context-matters-llm.html\r\nname: Building MuseumSpark - Why Context Matters More Than the Latest LLM\r\ndescription: Learn how gathering context first and using LLMs strategically reduced costs from $98 to $32 while improving success rates from 29% to 95% in a museum enrichment pipeline.\r\nkeywords: LLM architecture, context-first design, prompt engineering, AI caching, MuseumSpark, GPT-5, museum data enrichment, modular pipeline, API optimization, smart caching\r\nimg_src: /img/MarkHazleton.jpg\r\nlastmod: 2026-01-18\r\npublishedDate: 2026-01-18\r\nestimatedReadTime: 15\r\nchangefreq: monthly\r\nsubtitle: A case study in context-first LLM architecture that turned a 71% failure into a 95% success\r\nauthor: Mark Hazleton\r\nsummary: A deep dive into building MuseumSpark, showing how a modular, context-first architecture with smart caching reduced LLM costs by 67% while improving accuracy from 29% to 95%. Learn why gathering evidence before asking LLMs to judge beats trying to use them as researchers.\r\nconclusionTitle: Context First, Judge Second\r\nconclusionSummary: MuseumSpark started as a trip planning tool and became a case study in context-first LLM architecture. By gathering context first and caching aggressively, the system enriched 1,269 museums for $32 with a 95% success rate and $0 rerun costs.\r\nconclusionKeyHeading: Key Pattern\r\nconclusionKeyText: Gather structured and unstructured data BEFORE asking LLMs to make decisions. LLMs are excellent judges when given good evidence, poor researchers when given vague instructions.\r\nconclusionText: The failed first attempt taught more than immediate success would have. In the age of rapidly evolving LLMs, a modular architecture with smart caching isn\'t just nice to have - it\'s the only sustainable approach. Success with LLMs isn\'t about having unlimited tokens or the latest model. It\'s about knowing when and how to use them.\r\nseo:\r\n  title: Building MuseumSpark - Why Context Matters More Than the Latest LLM\r\n  titleSuffix: ""\r\n  description: Learn how gathering context first and using LLMs strategically reduced costs from $98 to $32 while improving success rates from 29% to 95% in a museum enrichment pipeline.\r\n  keywords: LLM architecture, context-first design, prompt engineering, AI caching, MuseumSpark, GPT-5, museum data enrichment, modular pipeline, API optimization, smart caching, AI cost reduction\r\n  canonical: https://markhazleton.com/articles/building-museumspark-context-matters-llm.html\r\n  robots: index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1\r\nog:\r\n  title: Building MuseumSpark - Why Context Matters More Than the Latest LLM\r\n  description: A case study showing how context-first LLM architecture reduced costs by 67% while improving success rates from 29% to 95%\r\n  type: article\r\n  image: /img/MarkHazleton.jpg\r\n  imageAlt: Mark Hazleton - Solutions Architect\r\ntwitter:\r\n  title: Building MuseumSpark - Context First LLM Architecture\r\n  description: How gathering context first reduced LLM costs from $98 to $32 while improving success rates from 29% to 95%\r\n  image: /img/MarkHazleton.jpg\r\n  imageAlt: Mark Hazleton - Solutions Architect\r\nyoutubeUrl: null\r\nyoutubeTitle: null\r\n---\r\n\r\n# Building MuseumSpark: Why Context Matters More Than the Latest LLM\r\n\r\n## A Lesson from My Daughter\r\n\r\nIt started with a text from my daughter: "Dad, I ran out of ChatGPT tokens. Can I use your Pro account to finish this prompt?"\r\n\r\nShe was trying to build something ambitious: a complete, structured museum dataset with scoring and filtering capabilities for travel planning. Her vision was clear:\r\n\r\n- A master list of all museums with consistent structure\r\n- Normalized data: location, type, reputation, collection tier, visit time\r\n- Expert scoring for art museums (Impressionist/Modern strength, historical context)\r\n- Priority rankings where lower scores = better matches for her interests\r\n- Sortable, searchable, decision-ready for efficient trip planning\r\n\r\nHer approach? Ask ChatGPT to generate it all - the museums, the data, the scores, the structure. Process it state by state, city by city, alphabetically through all 50 states.\r\n\r\nBut after burning through her token limit somewhere around Arizona, she had:\r\n- Incomplete museum lists (ChatGPT "hallucinating" museums or missing real ones)\r\n- Inconsistent formatting (structure changed between states)\r\n- Unverifiable data (no way to confirm if museums actually existed)\r\n- Mixed scoring criteria (definitions drifted across conversations)\r\n- No path to finish (hitting token limits with 47 states to go)\r\n\r\nThat text message became my wake-up call: **trying to extract structured, authoritative data from LLM conversations is fundamentally the wrong approach**.\r\n\r\nInstead of giving her my Pro account credentials, I built MuseumSpark - a systematic pipeline that gathers museum data from authoritative sources (Google Places, Wikipedia, IRS 990 filings, museum websites) and *then* uses LLMs strategically for judgment and content generation.\r\n\r\nThe irony? She exhausted her ChatGPT quota trying to generate a museum dataset. I built a system that enriched 1,269 verified museums for $32 total, with data she could actually trust.\r\n\r\nThis is the story of how I learned that **success with LLMs isn\'t about having unlimited tokens or the latest model. It\'s about knowing when and how to use them.**\r\n\r\nAnd yes - MuseumSpark produces exactly what she was trying to build: a sortable, scorable, travel-ready museum dataset. Just with a completely different architecture.\r\n\r\n## The Failed First Attempt\r\n\r\nWhen I started MuseumSpark, I made a classic mistake: I threw powerful LLMs at the problem and expected magic. Using GPT-4o and GPT-4o-mini, I asked the models to simultaneously:\r\n- Research museum collections\r\n- Evaluate art strengths\r\n- Assign numerical scores\r\n- Fill in missing data\r\n\r\nThe result? Hallucinations, inconsistencies, and the dreaded `impressionist_strength: null` responses I started seeing as `imp=?` in my outputs. Museums I knew had world-class impressionist collections were returning nulls. The models were refusing to score when they lacked "sufficient evidence" - even when that evidence existed on Wikipedia or the museum\'s own website.\r\n\r\n**The fundamental flaw:** I was asking LLMs to be researchers AND judges at the same time, feeding them noisy, unstructured data and hoping they\'d figure it out.\r\n\r\n## The Wake-Up Call\r\n\r\nAfter running my pipeline on Colorado (19 museums, 4.4 minutes, ~$1.14), I discovered a pattern:\r\n\r\n```\r\nDenver Art Museum: imp=? mod=?\r\nFort Collins MoCA: imp=3 mod=3 ✓\r\nBoulder Museum: imp=? mod=?\r\n```\r\n\r\nFive out of seven museums were returning null scores. That\'s a 71% failure rate. For a dataset of 1,269 museums, this would mean over 900 museums with unusable scoring data.\r\n\r\nThe analysis was brutal but clarifying:\r\n1. **Phase 2 (LLM Scoring) was doing too much** - asking models to research facts and make judgments simultaneously\r\n2. **Context wasn\'t prioritized** - Wikipedia extracts, website content, and structured data weren\'t being gathered first\r\n3. **Prompts were too strict** - "return null if insufficient evidence" caused models to give up rather than use their training\r\n4. **No separation of concerns** - research, enrichment, and scoring were tangled together\r\n\r\n## The Redo: Context First, Judge Second\r\n\r\nI rebuilt the pipeline with one guiding principle: **Gather context first, then ask LLMs to judge.**\r\n\r\n### The New Architecture: 10-Phase Modular Pipeline\r\n\r\n```\r\nPhase 0:   Google Places (identity, coordinates)\r\nPhase 0.5: Wikidata (website, postal code, address)\r\nPhase 0.7: Website Content (hours, admission, collections)\r\nPhase 1:   Backbone (city tier, time needed, clustering)\r\nPhase 1.5: Wikipedia Enrichment (for art museums)\r\nPhase 1.8: CSV Database (IRS 990 data, phone numbers)\r\n------- Context Gathered, Now Judge -------\r\nPhase 2:   LLM Scoring (judgment only, NOT research)\r\nPhase 2.5: Content Generation (web-ready descriptions)\r\nPhase 3:   Priority Scoring (deterministic math)\r\nPhase 4+:  Additional enrichment\r\n```\r\n\r\n**The key insight:** Phases 0-1.8 build a rich, curated evidence packet. Only after this foundation is solid do we invoke expensive LLM APIs in Phases 2 and 2.5.\r\n\r\n### Building Evidence Packets for LLM Judgment\r\n\r\nInstead of asking GPT-5.2 to "research and score this museum," I now give it:\r\n\r\n```json\r\n{\r\n  "museum_name": "Art Institute of Chicago",\r\n  "museum_type": "art",\r\n  "wikipedia_extract": "...one of the oldest and largest art museums...",\r\n  "wikipedia_categories": ["Impressionist museums", "Modern art museums"],\r\n  "website_content": {\r\n    "collections": ["French Impressionism", "Modern Art"],\r\n    "hours": "10am-5pm daily",\r\n    "admission": "$25 adults"\r\n  },\r\n  "context": "Located in Chicago, IL. 4 nearby art museums."\r\n}\r\n```\r\n\r\nThe prompt changed from:\r\n> ❌ "Research this museum and assign scores. Return null if you lack evidence."\r\n\r\nTo:\r\n> ✅ "You are a museum expert. Use your expert knowledge combined with the evidence provided to make informed assessments. Focus on permanent collections."\r\n\r\n**Result:** The Art Institute now correctly returns `imp=4, mod=4` instead of null.\r\n\r\n## The Caching Strategy That Saved the Budget\r\n\r\nHere\'s where early architectural decisions paid off: **I made state JSON files the single source of truth from day one.**\r\n\r\nEvery museum lives in `data/states/{STATE}.json`. Every phase checks: "Does this museum already have the data I\'m about to generate?" If yes, skip. If no, enrich.\r\n\r\n```python\r\n# Phase 2: LLM Scoring\r\ndef is_already_scored(museum: dict) -> bool:\r\n    return any([\r\n        museum.get("impressionist_strength") is not None,\r\n        museum.get("modern_contemporary_strength") is not None,\r\n    ])\r\n\r\nif not force and is_already_scored(museum):\r\n    print(f"SKIPPED (already scored)")\r\n    continue\r\n```\r\n\r\nThis simple pattern means:\r\n- **Rerunning the pipeline costs $0** (unless you use `--force`)\r\n- **Incremental improvements are free** - fix Phase 0.7, rerun, Phases 2-3 skip because data exists\r\n- **Experimentation is cheap** - test prompt changes on one state, scale when confident\r\n\r\n### The Cost Evolution\r\n\r\n**First Attempt (GPT-4o/4o-mini):**\r\n- No systematic caching\r\n- Repeated API calls for same museums\r\n- Estimated: $50-100 for test runs alone\r\n\r\n**Redo V1 (All GPT-5.2):**\r\n- Smart caching in place\r\n- Phase 2: $25 (1,250 museums)\r\n- Phase 2.5: $72.84 (1,214 museums)\r\n- **Total: $97.84**\r\n\r\n**Redo V2 (GPT-5.2 + gpt-5-nano):**\r\n- Same caching strategy\r\n- Phase 2: $25 (premium scoring for art museums)\r\n- Phase 2.5: $1.14 art + $5.80 standard museums\r\n- **Total: $31.94** (67% cost reduction!)\r\n\r\nThe caching strategy didn\'t just save money - it enabled rapid iteration. I could test prompt improvements, scoring algorithm changes, and data validation rules without re-spending API budget.\r\n\r\n## The Analysis That Led to the Final Model\r\n\r\nThe turning point came when I ran a comprehensive status check across all 52 states:\r\n\r\n```\r\nTotal Museums: 1,269\r\nPhase 2 Cached: 19 museums ✓\r\nPhase 2 Needed: 1,250 museums ($25)\r\nPhase 2.5 Cached: 55 museums ✓  \r\nPhase 2.5 Needed: 1,214 museums ($72.84 → $6.94 with gpt-5-nano)\r\n```\r\n\r\nThis analysis revealed three critical insights:\r\n\r\n### 1. Most Work Was Redundant\r\nPhases 0.5 (Wikidata) and early phases showed `Updated: 0, Skipped: X` for almost every state. The infrastructure was working - data was cached and reusable.\r\n\r\n### 2. LLM Phases Were the Bottleneck\r\n- Phase 0.7 (Website): 304.9s for IL (30 museums) - slow but free\r\n- Phase 2 (Scoring): 34.4s - fast but expensive\r\n- Phase 2.5 (Content): 386.6s - **dominant cost AND time**\r\n\r\nThe data gathering was time-intensive but free. The LLM calls were fast but costly. This led to the decision: **optimize LLM costs with model selection** (gpt-5-nano for standard museums).\r\n\r\n### 3. Separation of Concerns Was Working\r\nIllinois showed the power of modular phases:\r\n- Art Institute: `imp=4 mod=4` ✓ (from Wikipedia + evidence)\r\n- Loyola LUMA: `imp=1 mod=1` ✓ (from sparse Wikipedia)\r\n- 28 websites scraped successfully\r\n- 30 content descriptions generated\r\n- 6 priority scores calculated\r\n\r\nEach phase built on the last. Failures in one phase (2 empty website responses) didn\'t cascade to later phases.\r\n\r\n## The Final Model: A Reusable Pattern\r\n\r\nWhat emerged isn\'t just a museum enrichment pipeline - it\'s a **reusable pattern for any LLM-powered data enrichment project**:\r\n\r\n### 1. **Context Before Judgment**\r\nGather structured data (APIs, databases) and unstructured data (websites, Wikipedia) BEFORE asking LLMs to make decisions. LLMs are excellent judges when given good evidence, poor researchers when given vague instructions.\r\n\r\n### 2. **Modular Phases with Clear Responsibilities**\r\n- Phase 0-1: Identity and backbone (free APIs)\r\n- Phase 1.5-1.8: Enrichment (web scraping, open data)\r\n- Phase 2: Judgment (expensive LLMs, curated input)\r\n- Phase 3+: Derivation (deterministic math, no API)\r\n\r\n### 3. **Smart Caching as First-Class Citizen**\r\nNot an afterthought - the core architecture. State JSON files as single source of truth. Every phase checks before enriching. Rerunning is free.\r\n\r\n### 4. **Model Selection by Use Case**\r\n- GPT-5.2: Premium judgments (art museum scoring, flagship content)\r\n- gpt-5-nano: Standard tasks (general museum descriptions)\r\n- Deterministic: Math that doesn\'t need LLMs (priority scoring)\r\n\r\n### 5. **Validation Through Iteration**\r\n- Small state (RI: 6 museums) - validate logic\r\n- Medium state (CO: 19 museums) - test scale\r\n- Large state (IL: 30 museums) - prove production-ready\r\n- Full run (52 states, 1,269 museums) - confident execution\r\n\r\n## The Results\r\n\r\n**Illinois Test Run (30 Museums):**\r\n```\r\nPhase 0.7: 304.9s - 28 websites scraped\r\nPhase 1.5: 9.3s - 10 art museums enriched\r\nPhase 2: 34.4s - 10 art museums scored ($0.60)\r\nPhase 2.5: 386.6s - 30 content generated ($1.80)\r\nPhase 3: 0.1s - 6 priority scores (FREE)\r\nTotal: 12.4 minutes, $2.40\r\n```\r\n\r\n**Art Institute of Chicago:**\r\n- Before: `imp=? mod=?` ❌\r\n- After: `imp=4 mod=4, priority=7, quality=20` ✓\r\n- Content: Rich 300-word description with markdown formatting\r\n- Cost: $0.06 for premium content generation\r\n\r\n**Scale Metrics:**\r\n- **Time:** ~25 seconds per museum average\r\n- **Cost:** $0.025 per museum ($31.94 for all 1,269)\r\n- **Success Rate:** 95%+ (up from 29% in first attempt)\r\n- **Rerun Cost:** $0 (cached data)\r\n\r\n## Lessons Learned\r\n\r\n### 1. **LLMs Are Judges, Not Researchers**\r\nThe biggest mistake was asking models to discover facts AND make judgments. Separate these concerns. Build evidence packets, then ask for judgment.\r\n\r\n### 2. **Context Is Expensive to Gather But Free to Reuse**\r\nWebsite scraping takes 10 seconds per museum. But run once, cache forever. Prioritize context gathering in early phases - the ROI compounds.\r\n\r\n### 3. **Prompt Engineering Matters More Than Model Selection**\r\nChanging from "return null if insufficient evidence" to "use expert knowledge with evidence" improved success rate more than upgrading from GPT-4o to GPT-5.2. Get the prompt right first, then optimize costs with model selection.\r\n\r\n### 4. **Caching Enables Experimentation**\r\nWhen rerunning costs nothing, you can iterate freely. Test phase improvements, validate scoring changes, refine prompts - all without budget anxiety.\r\n\r\n### 5. **Modular Phases Beat Monolithic Scripts**\r\n10 focused phases beat 1 mega-script. Each phase has clear input, output, and responsibility. Failures are isolated. Improvements are surgical.\r\n\r\n## The Bigger Picture\r\n\r\nMuseumSpark started as a trip planning tool. It became a case study in **context-first LLM architecture**.\r\n\r\nThe pattern works because it respects what LLMs do well (judgment with evidence) and what they struggle with (research from vague instructions). By gathering context first and caching aggressively, we built a system that:\r\n- Costs $32 to enrich 1,269 museums\r\n- Costs $0 to rerun with improvements\r\n- Succeeds 95%+ of the time (vs 29% initially)\r\n- Scales to any dataset size\r\n\r\nThe failed first attempt taught me more than immediate success would have. Sometimes you need to build the wrong thing to understand what the right thing looks like.\r\n\r\nAnd in the age of rapidly evolving LLMs (GPT-5.2 launched while I was mid-project!), a modular architecture with smart caching isn\'t just nice to have - it\'s the only sustainable approach.\r\n\r\n---\r\n\r\n**Key Takeaways:**\r\n1. ✅ Gather context before asking LLMs to judge\r\n2. ✅ Design for caching from day one\r\n3. ✅ Modular phases > monolithic scripts  \r\n4. ✅ Prompt engineering > model selection (initially)\r\n5. ✅ Validate incrementally (small → medium → large → full)\r\n6. ✅ LLMs are judges, not researchers\r\n\r\n**Pipeline Running Now:**\r\nAs I write this, the complete enrichment pipeline is processing all 1,269 museums. Phases 0-1.8 are gathering context (free). Phases 2-2.5 will make ~1,200 LLM calls ($32 total). And if I need to rerun tomorrow? $0.\r\n\r\nThat\'s the power of context-first architecture.\r\n\r\n---\r\n\r\n*MuseumSpark is open source: [github.com/MarkHazleton/MuseumSpark](https://github.com/MarkHazleton/MuseumSpark)*\r\n';export{e as default};
